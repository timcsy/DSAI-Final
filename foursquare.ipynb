{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nmodel_path = '/kaggle/working/foursquare.pth'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-30T17:31:44.466803Z","iopub.execute_input":"2022-05-30T17:31:44.467244Z","iopub.status.idle":"2022-05-30T17:31:44.498097Z","shell.execute_reply.started":"2022-05-30T17:31:44.467161Z","shell.execute_reply":"2022-05-30T17:31:44.497033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom transformers import BertTokenizer\nfrom transformers import BertModel\nimport math\n\ndef distance(lat1, lon1, lat2, lon2):\n    if lat1 != lat1: lat1 = 0\n    if lon1 != lon1: lon1 = 0\n    if lat2 != lat2: lat2 = 0\n    if lon2 != lon2: lon2 = 0\n    lat1 = math.radians(lat1)\n    lon1 = math.radians(lon1)\n    lat2 = math.radians(lat2)\n    lon2 = math.radians(lon2)\n    value = math.sin(lon1) * math.sin(lon2) + math.cos(lon1) * math.cos(lon2) * math.cos(lat2 - lat1)\n    return abs(math.acos(np.clip(value, -1, 1))) * 100\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\nclass BertClassifier(nn.Module):\n\n    def __init__(self, dropout=0.5):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n        self.dropout = nn.Dropout(dropout)\n        self.linear1 = nn.Linear(768, 1)\n        self.linear2 = nn.Linear(3, 2)\n\n    def forward(self, data_input, input_id, mask):\n        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n        dropout_output = self.dropout(pooled_output)\n        linear1_output = self.linear1(dropout_output)\n        linear2_output = self.linear2(torch.cat([data_input, linear1_output], dim=1).float())\n\n        return linear2_output","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:32:13.262599Z","iopub.execute_input":"2022-05-30T17:32:13.262937Z","iopub.status.idle":"2022-05-30T17:32:13.313381Z","shell.execute_reply.started":"2022-05-30T17:32:13.262907Z","shell.execute_reply":"2022-05-30T17:32:13.312532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pairs = pd.read_csv('/kaggle/input/foursquare-location-matching/pairs.csv')\ndf_pairs","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:31:44.544058Z","iopub.execute_input":"2022-05-30T17:31:44.544424Z","iopub.status.idle":"2022-05-30T17:31:53.048951Z","shell.execute_reply.started":"2022-05-30T17:31:44.544393Z","shell.execute_reply":"2022-05-30T17:31:53.048038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nN = 10000\n\ndf_train, df_val, df_test = np.split(df_pairs.sample(n=N, random_state=42), \n                                     [int(.8 * N), int(.9 * N)])\n\nprint(len(df_train),len(df_val), len(df_test))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T17:32:13.133401Z","iopub.execute_input":"2022-05-30T17:32:13.134465Z","iopub.status.idle":"2022-05-30T17:32:13.259243Z","shell.execute_reply.started":"2022-05-30T17:32:13.134423Z","shell.execute_reply":"2022-05-30T17:32:13.258324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import Adam\nfrom tqdm import tqdm\n\ndef train(model, train_data, val_data, learning_rate, epochs):\n    train, val = Dataset(train_data), Dataset(val_data)\n\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=8)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    # use_cuda = False\n    # device = 'cpu'\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n\n    if use_cuda:\n        model = model.cuda()\n        criterion = criterion.cuda()\n\n    last_val_loss = torch.tensor(float(\"inf\"))\n\n    for epoch_num in range(epochs):\n        total_acc_train = 0\n        total_loss_train = 0\n        \n        with tqdm(train_dataloader) as pbar:\n            for train_input, train_label in pbar:\n                train_label = train_label.to(device)\n                data_input = train_input[0].to(device)\n                mask = train_input[1]['attention_mask'].to(device)\n                input_id = train_input[1]['input_ids'].squeeze(1).to(device)\n\n                output = model(data_input, input_id, mask)\n\n                batch_loss = criterion(output, train_label)\n                total_loss_train += batch_loss.item()\n\n                acc = (output.argmax(dim=1) == train_label.argmax(dim=1)).int().sum().item()\n                total_acc_train += acc\n\n                model.zero_grad()\n                batch_loss.backward()\n                optimizer.step()\n                pbar.set_postfix({'loss': batch_loss.item() / train_label.size()[0], 'acc': acc / train_label.size()[0]})\n        \n        total_acc_val = 0\n        total_loss_val = 0\n\n        with torch.no_grad():\n            for val_input, val_label in val_dataloader:\n                val_label = val_label.to(device)\n                data_input = val_input[0].to(device)\n                mask = val_input[1]['attention_mask'].to(device)\n                input_id = val_input[1]['input_ids'].squeeze(1).to(device)\n\n                output = model(data_input, input_id, mask)\n\n                batch_loss = criterion(output, val_label)\n                total_loss_val += batch_loss.item()\n                \n                acc = (output.argmax(dim=1) == val_label.argmax(dim=1)).int().sum().item()\n                total_acc_val += acc\n        \n        print(\n            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n        \n        if last_val_loss:\n            print(f'save model {model_path} from val_loss {last_val_loss} to {total_loss_val / len(val_data): .3f}')\n            torch.save(model.state_dict(), model_path)\n            last_val_loss = total_loss_val / len(val_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:02:34.478530Z","iopub.execute_input":"2022-05-30T18:02:34.478980Z","iopub.status.idle":"2022-05-30T18:02:34.497051Z","shell.execute_reply.started":"2022-05-30T18:02:34.478947Z","shell.execute_reply":"2022-05-30T18:02:34.496263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nLR = 1e-5\n\nmodel = BertClassifier()\nif os.path.exists(model_path):\n    model.load_state_dict(torch.load(model_path))\n\ntrain(model, df_train, df_val, LR, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T18:24:12.275022Z","iopub.execute_input":"2022-05-30T18:24:12.275400Z","iopub.status.idle":"2022-05-30T18:39:35.820760Z","shell.execute_reply.started":"2022-05-30T18:24:12.275369Z","shell.execute_reply":"2022-05-30T18:39:35.819897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertClassifier()\nmodel.load_state_dict(torch.load(model_path))\n\ndef is_match(x1, x2):\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    \n    with torch.no_grad():\n        dist = distance(x1['latitude'], x1['longitude'], x2['latitude'], x2['longitude'])\n        country = x1['country'] == x2['country']\n        text = tokenizer(\n            '' if x1['name'] != x1['name'] else x1['name'] + '[SEP]' +\n            '' if x1['categories'] != x1['categories'] else x1['categories'] + '[SEP]' +\n            '' if x2['name'] != x2['name'] else x2['name'] + '[SEP]' +\n            '' if x2['categories'] != x2['categories'] else x2['categories'] + '[SEP]',\n            padding='max_length', max_length = 512, truncation=True, return_tensors='pt'\n        )\n        \n        data_input = torch.tensor([[dist, country]]).to(device)\n        mask = text['attention_mask'].to(device)\n        input_id = text['input_ids'].squeeze(1).to(device)\n\n        output = model(data_input, input_id, mask)\n        \n        return (output[0][0] <= output[0][1]).item() and country\n\ndf = pd.read_csv('/kaggle/input/foursquare-location-matching/test.csv')\n\ndf_match = pd.DataFrame(columns=['id', 'matches'])\n\nfor i in range(len(df.index)):\n    df_match = df_match.append({'id': df.loc[i, 'id'], 'matches': df.loc[i, 'id']}, ignore_index=True)\n    for j in range(len(df.index)):\n        if i != j and is_match(df.loc[i,:], df.loc[j,:]):\n            df_match.loc[i, 'matches'] += ' ' + df.loc[j, 'id']\n\ndf_match.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T19:06:31.008272Z","iopub.execute_input":"2022-05-30T19:06:31.008711Z","iopub.status.idle":"2022-05-30T19:06:31.490454Z","shell.execute_reply.started":"2022-05-30T19:06:31.008673Z","shell.execute_reply":"2022-05-30T19:06:31.489545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For offline model\n!sudo apt-get install git-lfs\n!git lfs install\n!git clone https://huggingface.co/bert-base-multilingual-cased","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:38:30.866259Z","iopub.execute_input":"2022-05-30T20:38:30.867578Z"},"trusted":true},"execution_count":null,"outputs":[]}]}